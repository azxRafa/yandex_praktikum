{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Проект для «Викишоп»"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Интернет-магазин «Викишоп» запускает новый сервис. Теперь пользователи могут редактировать и дополнять описания товаров, как в вики-сообществах. То есть клиенты предлагают свои правки и комментируют изменения других. Магазину нужен инструмент, который будет искать токсичные комментарии и отправлять их на модерацию. \n",
    "\n",
    "Обучите модель классифицировать комментарии на позитивные и негативные. В вашем распоряжении набор данных с разметкой о токсичности правок.\n",
    "\n",
    "Постройте модель со значением метрики качества *F1* не меньше 0.75. \n",
    "\n",
    "**Инструкция по выполнению проекта**\n",
    "\n",
    "1. Загрузите и подготовьте данные.\n",
    "2. Обучите разные модели. \n",
    "3. Сделайте выводы.\n",
    "\n",
    "Для выполнения проекта применять *BERT* необязательно, но вы можете попробовать.\n",
    "\n",
    "**Описание данных**\n",
    "\n",
    "Данные находятся в файле `toxic_comments.csv`. Столбец *text* в нём содержит текст комментария, а *toxic* — целевой признак."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подготовка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords as nltk_stopwords\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "lem = WordNetLemmatizer()\n",
    "def lemmatization(text):\n",
    "    word_list = nltk.word_tokenize(text)\n",
    "    return ' '.join([lem.lemmatize(w) for w in word_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(text):\n",
    "    text = text.lower()    \n",
    "    text = re.sub(r\"(?:\\n|\\r)\", \" \", text)\n",
    "    text = re.sub(r\"[^a-zA-Z ]+\", \"\", text).strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_toxic = pd.read_csv(url, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  toxic\n",
       "0  Explanation\\nWhy the edits made under my usern...      0\n",
       "1  D'aww! He matches this background colour I'm s...      0\n",
       "2  Hey man, I'm really not trying to edit war. It...      0\n",
       "3  \"\\nMore\\nI can't make any real suggestions on ...      0\n",
       "4  You, sir, are my hero. Any chance you remember...      0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_toxic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_toxic.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Пропусков нет, очистим и леммантизируем текст в колонке `text`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_toxic['clean_text'] = df_toxic['text'].apply(clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_toxic['lemmatize_text'] = df_toxic['clean_text'].apply(lemmatization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>lemmatize_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>explanation why the edits made under my userna...</td>\n",
       "      <td>explanation why the edits made under my userna...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>daww he matches this background colour im seem...</td>\n",
       "      <td>daww he match this background colour im seemin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>hey man im really not trying to edit war its j...</td>\n",
       "      <td>hey man im really not trying to edit war it ju...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>more i cant make any real suggestions on impro...</td>\n",
       "      <td>more i cant make any real suggestion on improv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>you sir are my hero any chance you remember wh...</td>\n",
       "      <td>you sir are my hero any chance you remember wh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159446</th>\n",
       "      <td>\":::::And for the second time of asking, when ...</td>\n",
       "      <td>0</td>\n",
       "      <td>and for the second time of asking when your vi...</td>\n",
       "      <td>and for the second time of asking when your vi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159447</th>\n",
       "      <td>You should be ashamed of yourself \\n\\nThat is ...</td>\n",
       "      <td>0</td>\n",
       "      <td>you should be ashamed of yourself   that is a ...</td>\n",
       "      <td>you should be ashamed of yourself that is a ho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159448</th>\n",
       "      <td>Spitzer \\n\\nUmm, theres no actual article for ...</td>\n",
       "      <td>0</td>\n",
       "      <td>spitzer   umm theres no actual article for pro...</td>\n",
       "      <td>spitzer umm there no actual article for prosti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159449</th>\n",
       "      <td>And it looks like it was actually you who put ...</td>\n",
       "      <td>0</td>\n",
       "      <td>and it looks like it was actually you who put ...</td>\n",
       "      <td>and it look like it wa actually you who put on...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159450</th>\n",
       "      <td>\"\\nAnd ... I really don't think you understand...</td>\n",
       "      <td>0</td>\n",
       "      <td>and  i really dont think you understand  i cam...</td>\n",
       "      <td>and i really dont think you understand i came ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>159292 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text  toxic  \\\n",
       "0       Explanation\\nWhy the edits made under my usern...      0   \n",
       "1       D'aww! He matches this background colour I'm s...      0   \n",
       "2       Hey man, I'm really not trying to edit war. It...      0   \n",
       "3       \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "4       You, sir, are my hero. Any chance you remember...      0   \n",
       "...                                                   ...    ...   \n",
       "159446  \":::::And for the second time of asking, when ...      0   \n",
       "159447  You should be ashamed of yourself \\n\\nThat is ...      0   \n",
       "159448  Spitzer \\n\\nUmm, theres no actual article for ...      0   \n",
       "159449  And it looks like it was actually you who put ...      0   \n",
       "159450  \"\\nAnd ... I really don't think you understand...      0   \n",
       "\n",
       "                                               clean_text  \\\n",
       "0       explanation why the edits made under my userna...   \n",
       "1       daww he matches this background colour im seem...   \n",
       "2       hey man im really not trying to edit war its j...   \n",
       "3       more i cant make any real suggestions on impro...   \n",
       "4       you sir are my hero any chance you remember wh...   \n",
       "...                                                   ...   \n",
       "159446  and for the second time of asking when your vi...   \n",
       "159447  you should be ashamed of yourself   that is a ...   \n",
       "159448  spitzer   umm theres no actual article for pro...   \n",
       "159449  and it looks like it was actually you who put ...   \n",
       "159450  and  i really dont think you understand  i cam...   \n",
       "\n",
       "                                           lemmatize_text  \n",
       "0       explanation why the edits made under my userna...  \n",
       "1       daww he match this background colour im seemin...  \n",
       "2       hey man im really not trying to edit war it ju...  \n",
       "3       more i cant make any real suggestion on improv...  \n",
       "4       you sir are my hero any chance you remember wh...  \n",
       "...                                                   ...  \n",
       "159446  and for the second time of asking when your vi...  \n",
       "159447  you should be ashamed of yourself that is a ho...  \n",
       "159448  spitzer umm there no actual article for prosti...  \n",
       "159449  and it look like it wa actually you who put on...  \n",
       "159450  and i really dont think you understand i came ...  \n",
       "\n",
       "[159292 rows x 4 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_toxic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, test_y = train_test_split(df_toxic.drop(['text', 'toxic', 'clean_text'],axis=1),\n",
    "                                                    df_toxic['toxic'], \n",
    "                                                    test_size=0.2, stratify=df_toxic['toxic'],\n",
    "                                                    random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lemmatize_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>49286</th>\n",
       "      <td>question i believe that ive understood and cor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92850</th>\n",
       "      <td>before returning home and being disbanded in june</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138647</th>\n",
       "      <td>i have respectfully appended a reassurance to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2562</th>\n",
       "      <td>a i mentioned in the edit summary again did yo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6835</th>\n",
       "      <td>hi dutchbloke i said might give reason to pres...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63165</th>\n",
       "      <td>one will undoubtedly come out a soon a the dem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134039</th>\n",
       "      <td>i wa reasonable with him they got a week notic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63145</th>\n",
       "      <td>where the four grouping are actually found</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93913</th>\n",
       "      <td>can you source that like i did with mine becau...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107981</th>\n",
       "      <td>i am joeys publicist i asked jason to make the...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>127433 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           lemmatize_text\n",
       "49286   question i believe that ive understood and cor...\n",
       "92850   before returning home and being disbanded in june\n",
       "138647  i have respectfully appended a reassurance to ...\n",
       "2562    a i mentioned in the edit summary again did yo...\n",
       "6835    hi dutchbloke i said might give reason to pres...\n",
       "...                                                   ...\n",
       "63165   one will undoubtedly come out a soon a the dem...\n",
       "134039  i wa reasonable with him they got a week notic...\n",
       "63145          where the four grouping are actually found\n",
       "93913   can you source that like i did with mine becau...\n",
       "107981  i am joeys publicist i asked jason to make the...\n",
       "\n",
       "[127433 rows x 1 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Сделаем TF-IDF векторизацию"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(nltk_stopwords.words('english'))\n",
    "# tfidf_vectorizer = TfidfVectorizer(stop_words=stop_words, ngram_range=(1,1))\n",
    "# X_train_vectorizer = tfidf_vectorizer.fit_transform(X_train['lemmatize_text'].astype('U'))\n",
    "# X_test_vectorizer =  tfidf_vectorizer.transform(X_test['lemmatize_text'].astype('U'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pipe_svc = Pipeline([('vectorizer', TfidfVectorizer(stop_words=stop_words, ngram_range=(1,1))),\n",
    "                     ('model', LinearSVC(random_state=42))])\n",
    "grid_params_svc = [{'model__max_iter' :[500, 1000],\n",
    "                    'model__C': [1, 7, 9]}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs = -1\n",
    "\n",
    "SVC = GridSearchCV(estimator=pipe_svc,\n",
    "                   param_grid=grid_params_svc,\n",
    "                   scoring='f1',\n",
    "                   cv=10,\n",
    "                   n_jobs=jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10,\n",
       "             estimator=Pipeline(steps=[('vectorizer',\n",
       "                                        TfidfVectorizer(stop_words={'a',\n",
       "                                                                    'about',\n",
       "                                                                    'above',\n",
       "                                                                    'after',\n",
       "                                                                    'again',\n",
       "                                                                    'against',\n",
       "                                                                    'ain',\n",
       "                                                                    'all', 'am',\n",
       "                                                                    'an', 'and',\n",
       "                                                                    'any',\n",
       "                                                                    'are',\n",
       "                                                                    'aren',\n",
       "                                                                    \"aren't\",\n",
       "                                                                    'as', 'at',\n",
       "                                                                    'be',\n",
       "                                                                    'because',\n",
       "                                                                    'been',\n",
       "                                                                    'before',\n",
       "                                                                    'being',\n",
       "                                                                    'below',\n",
       "                                                                    'between',\n",
       "                                                                    'both',\n",
       "                                                                    'but', 'by',\n",
       "                                                                    'can',\n",
       "                                                                    'couldn',\n",
       "                                                                    \"couldn't\", ...})),\n",
       "                                       ('model', LinearSVC(random_state=42))]),\n",
       "             n_jobs=-1,\n",
       "             param_grid=[{'model__C': [1, 7, 9],\n",
       "                          'model__max_iter': [500, 1000]}],\n",
       "             scoring='f1')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SVC.fit(X_train['lemmatize_text'], y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'model__C': 1, 'model__max_iter': 500}, 0.775550132219754)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(SVC.best_params_, SVC.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pipe_lr = Pipeline([('tfidf', TfidfVectorizer(stop_words=stop_words, ngram_range=(1,1))),\n",
    "                    ('clf', LogisticRegression(random_state=42))])\n",
    "grid_params_lr = [{'clf__C': [1, 7, 9],\n",
    "                    'clf__solver': ['lbfgs','liblinear'],\n",
    "                    'clf__max_iter': [500,1000]}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "LR = GridSearchCV(estimator=pipe_lr,\n",
    "                   param_grid=grid_params_lr,\n",
    "                   scoring='f1',\n",
    "                   cv=10,\n",
    "                   n_jobs=jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10,\n",
       "             estimator=Pipeline(steps=[('tfidf',\n",
       "                                        TfidfVectorizer(stop_words={'a',\n",
       "                                                                    'about',\n",
       "                                                                    'above',\n",
       "                                                                    'after',\n",
       "                                                                    'again',\n",
       "                                                                    'against',\n",
       "                                                                    'ain',\n",
       "                                                                    'all', 'am',\n",
       "                                                                    'an', 'and',\n",
       "                                                                    'any',\n",
       "                                                                    'are',\n",
       "                                                                    'aren',\n",
       "                                                                    \"aren't\",\n",
       "                                                                    'as', 'at',\n",
       "                                                                    'be',\n",
       "                                                                    'because',\n",
       "                                                                    'been',\n",
       "                                                                    'before',\n",
       "                                                                    'being',\n",
       "                                                                    'below',\n",
       "                                                                    'between',\n",
       "                                                                    'both',\n",
       "                                                                    'but', 'by',\n",
       "                                                                    'can',\n",
       "                                                                    'couldn',\n",
       "                                                                    \"couldn't\", ...})),\n",
       "                                       ('clf',\n",
       "                                        LogisticRegression(random_state=42))]),\n",
       "             n_jobs=-1,\n",
       "             param_grid=[{'clf__C': [1, 7, 9], 'clf__max_iter': [500, 1000],\n",
       "                          'clf__solver': ['lbfgs', 'liblinear']}],\n",
       "             scoring='f1')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LR.fit(X_train['lemmatize_text'], y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Выводы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_logistic = LR.predict(X_test['lemmatize_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7760524499654934"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(test_y, predict_logistic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7814478863597466"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_SVC = SVC.predict(X_test['lemmatize_text'])\n",
    "f1_score(test_y, predict_SVC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LinearSVC</th>\n",
       "      <th>LogisticRegression</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>F1_score</th>\n",
       "      <td>0.781448</td>\n",
       "      <td>0.776052</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          LinearSVC  LogisticRegression\n",
       "F1_score   0.781448            0.776052"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame({'LinearSVC': [f1_score(test_y, predict_SVC)], 'LogisticRegression': [f1_score(test_y, predict_logistic)]}, index=[\"F1_score\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Вывод\n",
    "- Данные были очищены и лемматизированны.\n",
    "- Применена TF-IDF векторизация.\n",
    "- Удалось достич F1 больше 0.75 с LinearSVC и LogisticRegression, но LinearSVC показал лучшую точность."
   ]
  }
 ],
 "metadata": {
  "ExecuteTimeLog": [
   {
    "duration": 65480,
    "start_time": "2022-07-25T08:41:04.045Z"
   },
   {
    "duration": 2283,
    "start_time": "2022-07-25T08:42:09.527Z"
   },
   {
    "duration": 173,
    "start_time": "2022-07-25T08:42:11.811Z"
   },
   {
    "duration": 81942,
    "start_time": "2022-08-02T19:15:40.459Z"
   },
   {
    "duration": 0,
    "start_time": "2022-08-02T19:17:02.405Z"
   },
   {
    "duration": 80212,
    "start_time": "2022-08-02T19:17:17.530Z"
   },
   {
    "duration": 715,
    "start_time": "2022-08-30T17:11:52.526Z"
   },
   {
    "duration": 0,
    "start_time": "2022-08-30T17:11:53.243Z"
   },
   {
    "duration": 0,
    "start_time": "2022-08-30T17:11:53.244Z"
   },
   {
    "duration": 791,
    "start_time": "2022-08-30T17:12:08.082Z"
   },
   {
    "duration": 519,
    "start_time": "2022-08-30T17:12:15.613Z"
   },
   {
    "duration": 3,
    "start_time": "2022-08-30T17:12:18.323Z"
   },
   {
    "duration": 2250,
    "start_time": "2022-08-30T17:12:19.131Z"
   },
   {
    "duration": 14,
    "start_time": "2022-08-30T17:12:22.235Z"
   },
   {
    "duration": 849,
    "start_time": "2022-08-30T17:12:37.582Z"
   },
   {
    "duration": 7,
    "start_time": "2022-08-30T17:12:38.982Z"
   },
   {
    "duration": 42,
    "start_time": "2022-08-30T17:13:05.906Z"
   },
   {
    "duration": 10,
    "start_time": "2022-08-30T17:13:10.393Z"
   },
   {
    "duration": 216,
    "start_time": "2022-08-30T17:13:18.713Z"
   },
   {
    "duration": 1102,
    "start_time": "2022-08-30T17:13:33.493Z"
   },
   {
    "duration": 13,
    "start_time": "2022-08-30T17:14:13.450Z"
   },
   {
    "duration": 5,
    "start_time": "2022-08-30T17:14:19.052Z"
   },
   {
    "duration": 3,
    "start_time": "2022-08-30T17:14:22.243Z"
   },
   {
    "duration": 13,
    "start_time": "2022-08-30T17:14:34.092Z"
   },
   {
    "duration": 672,
    "start_time": "2022-08-30T17:14:47.974Z"
   },
   {
    "duration": 14,
    "start_time": "2022-08-30T17:15:28.403Z"
   },
   {
    "duration": 9,
    "start_time": "2022-08-30T17:15:39.372Z"
   },
   {
    "duration": 9,
    "start_time": "2022-08-30T17:15:57.118Z"
   },
   {
    "duration": 10,
    "start_time": "2022-08-30T17:16:09.199Z"
   },
   {
    "duration": 4,
    "start_time": "2022-08-30T17:16:12.832Z"
   },
   {
    "duration": 5,
    "start_time": "2022-08-30T17:16:28.098Z"
   },
   {
    "duration": 4,
    "start_time": "2022-08-30T17:17:07.947Z"
   },
   {
    "duration": 5,
    "start_time": "2022-08-30T17:17:30.514Z"
   },
   {
    "duration": 5,
    "start_time": "2022-08-30T17:17:35.035Z"
   },
   {
    "duration": 153,
    "start_time": "2022-08-30T17:22:41.610Z"
   },
   {
    "duration": 3,
    "start_time": "2022-08-30T17:23:12.277Z"
   },
   {
    "duration": 31,
    "start_time": "2022-08-30T17:23:26.410Z"
   },
   {
    "duration": 4,
    "start_time": "2022-08-30T17:23:49.368Z"
   },
   {
    "duration": 23,
    "start_time": "2022-08-30T17:23:57.339Z"
   },
   {
    "duration": 3,
    "start_time": "2022-08-30T17:24:32.255Z"
   },
   {
    "duration": 1425,
    "start_time": "2022-08-30T17:24:34.480Z"
   },
   {
    "duration": 27,
    "start_time": "2022-08-30T17:25:44.513Z"
   },
   {
    "duration": 116783,
    "start_time": "2022-08-30T17:25:54.955Z"
   },
   {
    "duration": 11,
    "start_time": "2022-08-30T17:28:34.094Z"
   },
   {
    "duration": 3,
    "start_time": "2022-08-30T17:29:44.310Z"
   },
   {
    "duration": 244,
    "start_time": "2022-08-30T17:30:57.167Z"
   },
   {
    "duration": 2,
    "start_time": "2022-08-30T17:31:05.376Z"
   },
   {
    "duration": 2415,
    "start_time": "2022-08-30T17:31:07.712Z"
   },
   {
    "duration": 11,
    "start_time": "2022-08-30T17:31:11.073Z"
   },
   {
    "duration": 13,
    "start_time": "2022-08-30T17:33:08.966Z"
   },
   {
    "duration": 12,
    "start_time": "2022-08-30T20:11:03.733Z"
   },
   {
    "duration": 102,
    "start_time": "2022-08-30T20:21:20.535Z"
   },
   {
    "duration": 8,
    "start_time": "2022-08-30T20:21:24.663Z"
   },
   {
    "duration": 9,
    "start_time": "2022-08-30T20:22:39.977Z"
   },
   {
    "duration": 449,
    "start_time": "2022-08-30T20:23:02.603Z"
   },
   {
    "duration": 11,
    "start_time": "2022-08-30T20:23:07.251Z"
   },
   {
    "duration": 3,
    "start_time": "2022-08-30T20:23:29.694Z"
   },
   {
    "duration": 10,
    "start_time": "2022-08-30T20:23:34.519Z"
   },
   {
    "duration": 6,
    "start_time": "2022-08-30T20:23:49.009Z"
   },
   {
    "duration": 23,
    "start_time": "2022-08-30T20:23:53.250Z"
   },
   {
    "duration": 3,
    "start_time": "2022-08-30T20:23:59.499Z"
   },
   {
    "duration": 28,
    "start_time": "2022-08-30T20:25:09.243Z"
   },
   {
    "duration": 14,
    "start_time": "2022-08-30T20:28:55.735Z"
   },
   {
    "duration": 2,
    "start_time": "2022-08-30T20:30:26.162Z"
   },
   {
    "duration": 15,
    "start_time": "2022-08-30T20:30:30.171Z"
   },
   {
    "duration": 8,
    "start_time": "2022-08-30T20:30:44.500Z"
   },
   {
    "duration": 7,
    "start_time": "2022-08-30T20:30:56.605Z"
   },
   {
    "duration": 10,
    "start_time": "2022-08-30T20:31:32.443Z"
   },
   {
    "duration": 5,
    "start_time": "2022-08-30T20:31:35.803Z"
   },
   {
    "duration": 4,
    "start_time": "2022-08-30T20:32:14.936Z"
   },
   {
    "duration": 7,
    "start_time": "2022-08-30T20:32:24.328Z"
   },
   {
    "duration": 32,
    "start_time": "2022-08-30T20:33:09.974Z"
   },
   {
    "duration": 6,
    "start_time": "2022-08-30T20:33:10.799Z"
   },
   {
    "duration": 28,
    "start_time": "2022-08-30T20:34:27.375Z"
   },
   {
    "duration": 6,
    "start_time": "2022-08-30T20:34:27.921Z"
   },
   {
    "duration": 7,
    "start_time": "2022-08-30T20:35:14.902Z"
   },
   {
    "duration": 13,
    "start_time": "2022-08-30T20:35:34.537Z"
   },
   {
    "duration": 7,
    "start_time": "2022-08-30T20:35:35.208Z"
   },
   {
    "duration": 4970,
    "start_time": "2022-08-30T20:35:45.930Z"
   },
   {
    "duration": 4212,
    "start_time": "2022-08-30T20:35:50.902Z"
   },
   {
    "duration": 7227,
    "start_time": "2022-08-30T20:36:12.510Z"
   },
   {
    "duration": 3,
    "start_time": "2022-08-30T20:36:24.046Z"
   },
   {
    "duration": 14,
    "start_time": "2022-08-30T20:36:24.231Z"
   },
   {
    "duration": 4,
    "start_time": "2022-08-30T20:36:43.777Z"
   },
   {
    "duration": 21965,
    "start_time": "2022-08-30T20:36:59.605Z"
   },
   {
    "duration": 1551,
    "start_time": "2022-08-30T20:37:21.572Z"
   },
   {
    "duration": 45,
    "start_time": "2022-08-30T20:37:49.538Z"
   },
   {
    "duration": 1333,
    "start_time": "2022-08-30T20:37:55.124Z"
   },
   {
    "duration": 3,
    "start_time": "2022-08-30T20:37:56.459Z"
   },
   {
    "duration": 6,
    "start_time": "2022-08-30T20:37:56.464Z"
   },
   {
    "duration": 2298,
    "start_time": "2022-08-30T20:37:56.471Z"
   },
   {
    "duration": 13,
    "start_time": "2022-08-30T20:37:58.771Z"
   },
   {
    "duration": 2456,
    "start_time": "2022-08-30T20:37:58.785Z"
   },
   {
    "duration": 75420,
    "start_time": "2022-08-30T20:38:01.242Z"
   },
   {
    "duration": 12,
    "start_time": "2022-08-30T20:39:16.663Z"
   },
   {
    "duration": 117,
    "start_time": "2022-08-30T20:39:16.677Z"
   },
   {
    "duration": 8,
    "start_time": "2022-08-30T20:39:16.796Z"
   },
   {
    "duration": 23256,
    "start_time": "2022-08-30T20:39:16.806Z"
   },
   {
    "duration": 1680,
    "start_time": "2022-08-30T20:39:40.064Z"
   },
   {
    "duration": 3,
    "start_time": "2022-08-30T20:39:41.746Z"
   },
   {
    "duration": 135,
    "start_time": "2022-08-30T20:39:41.751Z"
   },
   {
    "duration": 119826,
    "start_time": "2022-08-30T20:39:46.128Z"
   },
   {
    "duration": 14,
    "start_time": "2022-08-30T20:46:29.516Z"
   },
   {
    "duration": 6,
    "start_time": "2022-08-30T20:46:34.907Z"
   },
   {
    "duration": 12,
    "start_time": "2022-08-30T20:46:38.276Z"
   },
   {
    "duration": 601,
    "start_time": "2022-08-30T20:46:47.702Z"
   },
   {
    "duration": 598,
    "start_time": "2022-08-30T20:47:02.144Z"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Содержание",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "302.391px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}